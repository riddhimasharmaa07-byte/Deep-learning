# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/154_xlahFLLEyN6Z6nStaRINMNFGKJ6r8
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#create simple dataset manually
data_set={
    'sqt_living': [650,800,1000,1200,1400,1600,1800,2000,2200,2400],
    'price': [100000,120000,140000,160000,180000,200000,220000,240000,260000,300000]
}

#convert to dataframe
home_data=pd.DataFrame(data_set)

print('sample dataset created successfully')
print(home_data)

#prepare feature(x) and target(y)
X=home_data[['sqt_living']].values #features must be 2D
y=home_data['price'].values #Target(1D)

# splittin the dataset into training and test set

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=1/3,random_state=0)
from sklearn.linear_model import LinearRegression
Jitendratyagi = LinearRegression()
Jitendratyagi.fit(X_train, y_train)

#prediction of test and training set results
y_pred=Jitendratyagi.predict(X_test)
y_pred_train=Jitendratyagi.predict(X_train)

#print model parameters and simple metrics
print("\nmodel slope(coefficient):",Jitendratyagi.coef_)
print("model intercept:",Jitendratyagi.intercept_)

from sklearn.metrics import mean_squared_error,r2_score
print("\nTrain R^2:",r2_score(y_train,y_pred_train))
print("Test R^2:",r2_score(y_test,y_pred))
print("Train RMSE:",np.sqrt(mean_squared_error(y_train,y_pred_train)))

#visualize the training results with a smooth regression line

plt.figure(figsize=(8,5))

# create smooth line for regression
line_x = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
line_y = Jitendratyagi.predict(line_x)
plt.plot(line_x, line_y, label='Regression line', color='green', linewidth=2)

# visualize training data points
plt.scatter(X_train, y_train, label='Training data', color='blue')

plt.title('House price vs living area(Training set)')
plt.xlabel('Living area (sqft) ')
plt.ylabel('House price ($)')
plt.legend()
plt.grid(True)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

x=load_iris().data
y=load_iris().target

load_iris().target

x_train, x_test , y_train , y_test = train_test_split(x,y,test_size=0.2,random_state=42)
model = LogisticRegression()
model.fit(x_train,y_train)
predictions = model.predict(x_test)
print("Accuracy:", accuracy_score(y_test, predictions))
print("Classification Report:\n", classification_report(y_test, predictions))
print("Confusion Matrix:\n", confusion_matrix(y_test, predictions))

# Visualize the decision boundaries
# Using only the first two features for simplicity in visualization
X_vis = x[:, :2]
y_vis = y

x_min, x_max = X_vis[:, 0].min() - .5, X_vis[:, 0].max() + .5
y_min, y_max = X_vis[:, 1].min() - .5, X_vis[:, 1].max() + .5
h = .02  # step size in the mesh
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# Train a logistic regression model on the first two features for visualization
model_vis = LogisticRegression()
model_vis.fit(X_vis, y_vis)

Z = model_vis.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.figure(1, figsize=(8, 6))
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)

# Plot also the training points
plt.scatter(X_vis[:, 0], X_vis[:, 1], c=y_vis, edgecolors='k', cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.title('Logistic Regression Decision Boundaries (using Sepal Length and Sepal Width)')

plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xticks(())
plt.yticks(())

plt.show()

import pandas as pd

#create your own dataset
data={
    'study_hours':[1,2,3,4,5,6,7,8,9,10],
    'attendance(%)':[55,60,65,70,75,80,85,90,95,100],
    'internal_marks':[60,65,70,75,80,85,90,95,100,105],
    'result':['pass','pass','pass','pass','pass','pass','pass','pass','fail','fail']
}

#convert to dataframe
df = pd.DataFrame(data)

#show dataset
print("student performance dataset")
print(df)

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier , plot_tree
from sklearn.metrics import accuracy_score , classification_report , confusion_matrix
import matplotlib.pyplot as plt

#prepare data
x=df[['study_hours','attendance(%)','internal_marks']]
y=df['result']

#split data
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)

#train model
model=DecisionTreeClassifier(criterion='entropy',random_state=0)
model.fit(x_train,y_train)

#predict
y_pred=model.predict(x_test)

#evaluate
print("Accuracy:",accuracy_score(y_test,y_pred))
print("Classification Report:\n",classification_report(y_test,y_pred))
print("Confusion Matrix:\n",confusion_matrix(y_test,y_pred))

#visualize
plt.figure(figsize=(10,8))
plot_tree(model,feature_names=x.columns,class_names=['Fail','pass'],filled=True)
plt.show()

#random forests
#libraries
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt

#load the dataset
iris=load_iris()
print('df')
display(df)

#create dataframe for better visulaization
df=pd.DataFrame(data=iris.data,columns=iris.feature_names)
df['species']=iris.target
print('df')
display(df)

#create dataframe for better visulaization
df=pd.DataFrame(data=iris.data,columns=iris.feature_names)
df['species']=iris.target

#show dataset
print("iris dataset")
print(df.head())

#dataset info
print("\ndataset info")
print(df.info())
print(df.describe())
print(df.head())

#split data into feature and labels
x=df.iloc[:,:-1]
y=df.iloc[:,-1]

#split data (80%,20%)
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)

#model training
model=RandomForestClassifier(n_estimators=100,random_state=0)
model.fit(x_train,y_train)

# predict
y_pred=model.predict(x_test)

#evaluate
print("Accuracy:",accuracy_score(y_test,y_pred))
print("Classification Report:\n",classification_report(y_test,y_pred))
print("Confusion Matrix:\n",confusion_matrix(y_test,y_pred))

#visualize
plt.figure(figsize=(20,10))
plot_tree(model.estimators_[0],
          feature_names=x.columns,
          class_names=iris.target_names,
          filled=True,
          rounded=True,
          fontsize=10)
plt.title('Visualization of the First Decision Tree in the Random Forest')
plt.show()

#import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

#load the iris dataset
iris = load_iris()

#convert dataset to dataframe
df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
df['species'] = iris.target

#display first  5 rows
print(df.head())

# split into feature(x) and target(y)
x = df.iloc[:, :-1]
y = df.iloc[:, -1]

#split into training and testing set
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

#feature scaling
from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
x_train=sc.fit_transform(x_train)
x_test=sc.transform(x_test)

#create and train knn classifier
knn=KNeighborsClassifier(n_neighbors=3)
knn.fit(x_train,y_train)

#make predictions
y_pred=knn.predict(x_test)

#evaluate
print("Accuracy:",accuracy_score(y_test,y_pred))
print("Classification Report:\n",classification_report(y_test,y_pred))
print("Confusion Matrix:\n",confusion_matrix(y_test,y_pred))

#visualise
plt.figure(figsize=(8,6))
plt.scatter(x_test[:,0],x_test[:,1],c=y_pred,cmap='viridis',edgecolors='k',s=50,marker='x')
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')
plt.title('K-Nearest Neighbors Classification')
plt.show()

#Bagging

#ibraries implementation
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

#load dataset
iris=load_iris()
x,y=iris.data,iris.target

#split into train/test
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)

#create base model
base_model=DecisionTreeClassifier()

#create bagging model
bagging_model=BaggingClassifier(estimator=base_model,n_estimators=10,random_state=42)

#train both models
base_model.fit(x_train,y_train)
bagging_model.fit(x_train,y_train)

#compare both performance
base_pred=base_model.predict(x_test)
bagging_pred=bagging_model.predict(x_test)

# evaluation
print("Base Model Accuracy:",accuracy_score(y_test,base_pred))
print("Bagging Model Accuracy:",accuracy_score(y_test,bagging_pred))

#visualization
plt.figure(figsize=(10,6))
plt.scatter(x_test[:,0],x_test[:,1],c=bagging_pred,cmap='viridis',edgecolors='k',s=50,marker='x',label='Bagging')
plt.scatter(x_test[:,0],x_test[:,1],c=y_test,cmap='viridis',edgecolors='k',s=50,marker='o',label='Actual')
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')
plt.title('Bagging Classification')
plt.legend()
plt.show()

#stacking

#Libraries required
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

#define base learners
base_learners=[
    ('lr',LogisticRegression()),
    ('dt',DecisionTreeClassifier())
]

#meta learner
meta_learner=LogisticRegression()

#create stacking model
stack_model=StackingClassifier(estimators=base_learners,final_estimator=meta_learner)

#train
stack_model.fit(x_train,y_train)

#evaluate
y_pred=stack_model.predict(x_test)

#visualize
plt.figure(figsize=(10,6))
plt.scatter(x_test[:,0],x_test[:,1],c=y_pred,cmap='viridis',edgecolors='k',s=50,marker='x',label='Stacking')
plt.scatter(x_test[:,0],x_test[:,1],c=y_test,cmap='viridis',edgecolors='k',s=50,marker='o',label='Actual')
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')
plt.title('Stacking Classification')
plt.legend()
plt.show()

# BOOSTING (ada,gradient,XGBOOST) : Adaboost

#libraries required
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

#create base model
base_model=DecisionTreeClassifier(max_depth=1)

#create boosting model
boosting_model=AdaBoostClassifier(estimator=base_model,n_estimators=50,random_state=42)

#train
boosting_model.fit(x_train,y_train)

# Load and split the dataset
iris = load_iris()
x, y = iris.data, iris.target
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

#evaluate
y_pred=boosting_model.predict(x_test)

#evaluation
print("Accuracy:",accuracy_score(y_test,y_pred))
print("Classification Report:\n",classification_report(y_test,y_pred))
print("Confusion Matrix:\n",confusion_matrix(y_test,y_pred))

#visualization of adaboost
plt.figure(figsize=(10,6))
plt.scatter(x_test[:,0],x_test[:,1],c=y_pred,cmap='viridis',edgecolors='k',s=50,marker='x',label='AdaBoost')
plt.scatter(x_test[:,0],x_test[:,1],c=y_test,cmap='viridis',edgecolors='k',s=50,marker='o',label='Actual')
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')
plt.title('AdaBoost Classification')
plt.legend()
plt.show()

# BOOSTING (ada,gradient,XGBOOST) : Gradientboost

#libraries required
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

#create base model
base_model=DecisionTreeClassifier

#create boosting model
boosting_model=GradientBoostingClassifier(n_estimators=100,learning_rate=1.0,max_depth=1,random_state=42)

#splitting
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)

#train
boosting_model.fit(x_train,y_train)

#predicting
y_pred=boosting_model.predict(x_test)

#evaluation
print("Accuracy:",accuracy_score(y_test,y_pred))
print("Classification Report:\n",classification_report(y_test,y_pred))
print("Confusion Matrix:\n",confusion_matrix(y_test,y_pred))

#visualization
plt.figure(figsize=(10,6))
plt.scatter(x_test[:,0],x_test[:,1],c=y_pred,cmap='viridis',edgecolors='k',s=50,marker='X',label='GradientBoost')
plt.scatter(x_test[:,0],x_test[:,1],c=y_test,cmap='viridis',edgecolors='k',s=50,marker='o',label='Actual')
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')
plt.title('GradientBoost Classification')
plt.legend()
plt.show()

# BOOSTING (ada,gradient,XGBOOST) : XGboost

#libraries required
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

#create base model(weak learner)
base_model=DecisionTreeClassifier(max_depth=1)

#create boosting model
boosting_model=XGBClassifier(n_estimators=100,learning_rate=1.0,max_depth=1,random_state=42)

#train
boosting_model.fit(x_train,y_train)

#predict
y_pred=boosting_model.predict(x_test)

#evaluation
print("Accuracy:",accuracy_score(y_test,y_pred))
print("Classification Report:\n",classification_report(y_test,y_pred))
print("Confusion Matrix:\n",confusion_matrix(y_test,y_pred))

#visualization
plt.figure(figsize=(10,6))
plt.scatter(x_test[:,0],x_test[:,1],c=y_pred,cmap='viridis',edgecolors='k',s=50,marker='X',label='XGboost')
plt.scatter(x_test[:,0],x_test[:,1],c=y_test,cmap='viridis',edgecolors='k',s=50,marker='o',label='Actual')
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')
plt.title('XGboost Classification')
plt.legend()
plt.show()

#SVM

#libraries required
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

#load dataset
iris=load_iris()
x,y=iris.data,iris.target

#split data
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)

#train svm model
svm_model=SVC(kernel='linear',C=1.0)
svm_model.fit(x_train,y_train)

#predict and check accuracy
y_pred=svm_model.predict(x_test)
print("Accuracy:",accuracy_score(y_test,y_pred))

#visualize decision boundaries
# For visualization purposes, train a new SVM model using only the first two features
svm_model_vis = SVC(kernel='linear', C=1.0)
svm_model_vis.fit(x_train[:, :2], y_train)

x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1
y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))

# Predict on the meshgrid using the model trained on two features
z = svm_model_vis.predict(np.array([xx.ravel(), yy.ravel()]).T)
z = z.reshape(xx.shape)

plt.contourf(xx, yy, z, alpha=0.4, cmap='viridis')
plt.scatter(x[:, 0], x[:, 1], c=y, cmap='viridis', edgecolors='k', s=50, marker='o')
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')
plt.title('SVM Classification (using Sepal Length and Sepal Width)')
plt.show()

#tensorflow and keras
#libraries required

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

#create simple training data
x=np.array([[2,9],[1,5],[3,6],[4,8],[6,9],[5,5],[7,3]],dtype=float)
y=np.array([0,0,0,1,1,0,1],dtype=float)

#normalize data (important for neural network)
x=x / np.amax(x,axis=0)

#build the neural network model
model=keras.Sequential()
model.add(layers.Dense(10,input_dim=2,activation='relu'))
model.add(layers.Dense(1,activation='sigmoid'))

#compile the model
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

#train the model
model.fit(x,y,epochs=200,verbose=0)

#test the model with new output
test_data=np.array([[4,7]])
prediction=model.predict(test_data)

print("predicted output (1=pass,0=fail):",prediction)
if prediction > 0.5:
    print("Student will pass")
else:
    print("Student will fail")

#PCA

#libraries required
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

#load the dataset
iris=load_iris()
x,y=iris.data,iris.target

#apply pca(reduce 4d->2d)
pca=PCA(n_components=2)
x_pca=pca.fit_transform(x)

#visualize the result
plt.scatter(x_pca[:,0],x_pca[:,1],c=y,cmap='viridis',edgecolors='k',s=50,marker='o')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA Visualization')
plt.show()

#K-means clustering

#Libraries required
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans

#Create some sample data
x=np.array([[2,9],[1,5],[3,6],[4,8],[6,9],[5,5],[7,3]])
#Build k-means model
kmeans=KMeans(n_clusters=2,random_state=0)
kmeans.fit(x)

#Get cluster centers and labels
centroids = kmeans.cluster_centers_
labels=kmeans.labels_

#visualize the clusters
plt.scatter(x[:,0],x[:,1],c=labels,cmap='viridis',edgecolors='k',s=50,marker='o')
plt.scatter(centroids[:,0],centroids[:,1],c='red',marker='X',s=200,label='Centroids')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('K-means Clustering')
plt.show()

#hierarchical clustering
#importing libraries
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import linkage, dendrogram

#create sample data
x=np.array([[2,9],[1,5],[3,6],[4,8],[6,9],[5,5],[7,3]])

# perform hierarchial clustering
Z = linkage(X,method='ward')

#plot the dendrogram
plt.figure(figsize=(10,5))
dendrogram(Z,leaf_rotation=90,leaf_font_size=12)
plt.xlabel('Data Points')
plt.ylabel('Euclidean Distance')
plt.title('Dendrogram')
plt.show()

#form clusters
