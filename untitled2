# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nmmcG7oiFb4NUx343n3koA7fn6T-mIOx
"""

# Program 1: Linear Regression

# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Step 1: Create sample data
# X = study hours, Y = exam scores
X = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)  # feature must be 2D
Y = np.array([10, 20, 30, 40, 50, 60])

# Step 2: Create and train the Linear Regression model
model = LinearRegression()
model.fit(X, Y)

# Step 3: Make predictions
Y_pred = model.predict(X)

# Step 4: Display results
print("Coefficient (Slope):", model.coef_[0])
print("Intercept:", model.intercept_)
print("Equation of Line: Y = {:.2f}X + {:.2f}".format(model.coef_[0], model.intercept_))

# Step 5: Visualization
plt.scatter(X, Y, color='blue', label="Actual Data")
plt.plot(X, Y_pred, color='red', label="Best Fit Line")
plt.title("Linear Regression Example")
plt.xlabel("Study Hours")
plt.ylabel("Exam Score")
plt.legend()
plt.show()

# Program 2: Best Fit for Linear Regression

# Objective:
# Find the best-fit line (Y = mX + c) manually and visualize it.
# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt

# Step 1: Create sample data
X = np.array([1, 2, 3, 4, 5, 6])
Y = np.array([3, 4, 2, 5, 6, 7])

# Step 2: Compute slope (m) and intercept (c) manually
# Formula for slope (m) = Σ((x - mean_x)*(y - mean_y)) / Σ((x - mean_x)^2)
# Formula for intercept (c) = mean_y - m * mean_x
mean_x = np.mean(X)
mean_y = np.mean(Y)

# Numerator and Denominator for slope
m = np.sum((X - mean_x) * (Y - mean_y)) / np.sum((X - mean_x) ** 2)
c = mean_y - m * mean_x
print(f"Slope (m): {m:.2f}")
print(f"Intercept (c): {c:.2f}")
print(f"Equation of Best Fit Line: Y = {m:.2f}X + {c:.2f}")


# Step 3: Predict values using the best-fit line
Y_pred = m * X + c


# Step 4: Visualize the best-fit line
plt.scatter(X, Y, color='blue', label='Actual Data')
plt.plot(X, Y_pred, color='red', label='Best Fit Line')
plt.title("Best Fit Line using Manual Linear Regression")
plt.xlabel("Experience (Years)")
plt.ylabel("Salary (in $1000s)")
plt.legend()
plt.show()

# ============================================================
# Program 3: Logistic Regression
# ============================================================

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

X = np.array([[20, 80], [25, 60], [30, 70], [35, 50], [40, 40], [45, 30], [50, 20]])
Y = np.array([0, 0, 0, 1, 1, 1, 1])

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)

model = LogisticRegression()
model.fit(X_train, Y_train)

Y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(Y_test, Y_pred))
print("Confusion Matrix:\n", confusion_matrix(Y_test, Y_pred))
print("Classification Report:\n", classification_report(Y_test, Y_pred))

# ============================================================
# Program 4: Logistic Regression – New Product Purchase Dataset
# ============================================================

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

data = {
    'Age': [22, 25, 47, 52, 46, 56, 55, 60, 62, 61],
    'Salary': [25000, 29000, 47000, 60000, 52000, 65000, 64000, 72000, 75000, 80000],
    'Purchased': [0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
}
df = pd.DataFrame(data)

X = df[['Age', 'Salary']]
Y = df['Purchased']

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)

model = LogisticRegression()
model.fit(X_train, Y_train)

Y_pred = model.predict(X_test)

print("Predictions:", Y_pred)
print("Accuracy:", accuracy_score(Y_test, Y_pred))

# ============================================================
# Program 5: Multiple Linear Regression
# ============================================================

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

data = {
    'Experience': [1, 2, 3, 4, 5, 6, 7, 8],
    'Test_Score': [50, 55, 65, 70, 70, 75, 80, 85],
    'Interview_Score': [55, 60, 65, 70, 75, 80, 85, 90],
    'Salary': [40000, 45000, 50000, 55000, 60000, 65000, 70000, 75000]
}
df = pd.DataFrame(data)

X = df[['Experience', 'Test_Score', 'Interview_Score']]
Y = df['Salary']

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)

model = LinearRegression()
model.fit(X_train, Y_train)

Y_pred = model.predict(X_test)

print("Coefficients:", model.coef_)
print("Intercept:", model.intercept_)
print("R2 Score:", r2_score(Y_test, Y_pred))

# ============================================================
# Program 6: Multiple Linear Regression (Alternate Dataset)
# ============================================================

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

data = {
    'R&D_Spend': [165349.2, 162597.7, 153441.51, 144372.41, 142107.34],
    'Administration': [136897.8, 151377.59, 101145.55, 118671.85, 91391.77],
    'Marketing_Spend': [471784.1, 443898.53, 407934.54, 383199.62, 366168.42],
    'Profit': [192261.83, 191792.06, 191050.39, 182901.99, 166187.94]
}
df = pd.DataFrame(data)

X = df[['R&D_Spend', 'Administration', 'Marketing_Spend']]
Y = df['Profit']

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)

model = LinearRegression()
model.fit(X_train, Y_train)

Y_pred = model.predict(X_test)

print("Predicted Profit:", Y_pred)
print("MSE:", mean_squared_error(Y_test, Y_pred))
print("R2 Score:", r2_score(Y_test, Y_pred))

# ============================================================
# Program 7: K-Nearest Neighbors (KNN) – Accuracy Check
# ============================================================

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

iris = load_iris()
X = iris.data
Y = iris.target

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)

model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, Y_train)

Y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(Y_test, Y_pred))

# ============================================================
# Program 8: K-Nearest Neighbors (KNN) – Breast Cancer Dataset
# ============================================================

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

data = load_breast_cancer()
X = data.data
Y = data.target

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)

model = KNeighborsClassifier(n_neighbors=7)
model.fit(X_train, Y_train)

Y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(Y_test, Y_pred))
print("Confusion Matrix:\n", confusion_matrix(Y_test, Y_pred))
print("Classification Report:\n", classification_report(Y_test, Y_pred))

# ============================================================
# Program 9: K-Nearest Neighbors (KNN) from Scratch
# ============================================================

import numpy as np
from collections import Counter

def euclidean_distance(x1, x2):
    return np.sqrt(np.sum((x1 - x2) ** 2))

class KNN:
    def __init__(self, k=3):
        self.k = k

    def fit(self, X, y):
        self.X_train = X
        self.y_train = y

    def predict(self, X):
        predictions = [self._predict(x) for x in X]
        return np.array(predictions)

    def _predict(self, x):
        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]
        k_indices = np.argsort(distances)[:self.k]
        k_nearest_labels = [self.y_train[i] for i in k_indices]
        most_common = Counter(k_nearest_labels).most_common(1)
        return most_common[0][0]

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

iris = load_iris()
X = iris.data
Y = iris.target

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)

model = KNN(k=5)
model.fit(X_train, Y_train)

predictions = model.predict(X_test)

print("Accuracy:", accuracy_score(Y_test, predictions))

# ============================================================
# Program 10: Decision Tree Classifier
# ============================================================

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

iris = load_iris()
X = iris.data
Y = iris.target

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)

model = DecisionTreeClassifier(criterion='entropy', random_state=0)
model.fit(X_train, Y_train)

Y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(Y_test, Y_pred))

plt.figure(figsize=(10,6))
plot_tree(model, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)
plt.show()

# ============================================================
# Program 11: Naive Bayes Classifier
# ============================================================

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

iris = load_iris()
X = iris.data
Y = iris.target

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)

model = GaussianNB()
model.fit(X_train, Y_train)

Y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(Y_test, Y_pred))
print("Confusion Matrix:\n", confusion_matrix(Y_test, Y_pred))
print("Classification Report:\n", classification_report(Y_test, Y_pred))

# ============================================================
# Program 12: Support Vector Machine (SVM)
# ============================================================

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

iris = load_iris()
X = iris.data
Y = iris.target

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)

model = SVC(kernel='linear')
model.fit(X_train, Y_train)

Y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(Y_test, Y_pred))
print("Confusion Matrix:\n", confusion_matrix(Y_test, Y_pred))
print("Classification Report:\n", classification_report(Y_test, Y_pred))

# ============================================================
# Program 13: Principal Component Analysis (PCA)
# ============================================================

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

iris = load_iris()
X = iris.data
Y = iris.target

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(8,6))
plt.scatter(X_pca[:,0], X_pca[:,1], c=Y, cmap='viridis', edgecolors='k')
plt.title("PCA on Iris Dataset")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.show()

# ============================================================
# Program 14: Bagging Classifier
# ============================================================

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

iris = load_iris()
X = iris.data
Y = iris.target

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)

base_model = DecisionTreeClassifier()
model = BaggingClassifier(estimator=base_model, n_estimators=10, random_state=0)
model.fit(X_train, Y_train)

Y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(Y_test, Y_pred))

# ============================================================
# Program 15: Boosting (AdaBoost Classifier)
# ============================================================

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score

iris = load_iris()
X = iris.data
Y = iris.target

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)

model = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=0)
model.fit(X_train, Y_train)

Y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(Y_test, Y_pred))

# ============================================================
# Program 16: DBSCAN (Density-Based Spatial Clustering)
# ============================================================

from sklearn.datasets import make_moons
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt

X, _ = make_moons(n_samples=200, noise=0.05, random_state=0)

model = DBSCAN(eps=0.2, min_samples=5)
labels = model.fit_predict(X)

plt.figure(figsize=(8,6))
plt.scatter(X[:,0], X[:,1], c=labels, cmap='plasma', s=50)
plt.title("DBSCAN Clustering")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()
